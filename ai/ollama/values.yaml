ollama:
  gpu:
    enabled: false
  models:
    pull:
      - llama3:8b-instruct-q4_K_M
    run:
      - llama3:8b-instruct-q4_K_M
  nodeSelector:
    kubernetes.io/hostname: rpi-k8s-5

extraEnv:
  - name: OLLAMA_DEBUG
    value: "1"
  - name: OLLAMA_NUM_PARALLEL
    value: "12"
  - name: OLLAMA_NUM_CTX
    value: "4096"
  - name: OLLAMA_NUM_THREADS
    value: "12"
  - name: OLLAMA_NUM_BATCH
    value: "512"
  - name: OLLAMA_KEEP_ALIVE
    value: "5m"

podAnnotations:
  hugepages-2Mi: "1Gi"

resources:
  requests:
    cpu: "1"
    memory: "512Mi"
  limits:
    cpu: "12"
    memory: "16Gi"
# ingress:
#   enabled: true
#   className: nginx
#   annotations:
#     cert-manager.io/cluster-issuer: "cloudflare-prod"
#   hosts:
#     - host: ollama.atoca.house
#       paths:
#         - path: /
#           pathType: Prefix
#   tls:
#     - secretName: ollama-tls
#       hosts:
#         - ollama.atoca.house